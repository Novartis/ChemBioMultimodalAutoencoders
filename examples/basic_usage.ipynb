{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Usage Tutorial\n",
    "This notebook aims at showing how to use the multimodal translation model in its basic form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing the needed parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will need to import a few things first\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# changing working directory\n",
    "# for imports to work\n",
    "from pathlib import Path\n",
    "path = Path(os.getcwd())\n",
    "os.chdir(path.parent)\n",
    "\n",
    "# import the encoders and decoders we want to use\n",
    "from multimodal_autoencoders.model.encoders import DynamicEncoder\n",
    "from multimodal_autoencoders.model.decoders import DynamicDecoder\n",
    "\n",
    "# import the Autoencoder class\n",
    "from multimodal_autoencoders.base.autoencoder import VariationalAutoencoder\n",
    "\n",
    "# import a discriminator and a classifier\n",
    "from multimodal_autoencoders.model.classifiers import Discriminator, SimpleClassifier\n",
    "\n",
    "# import the JointTrainer aka the brains of the operation\n",
    "from multimodal_autoencoders.joint_trainer import JointTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up the data\n",
    " Therefore the model relies on simple numpy arrays as a data input. Please note that this data should be already processed the way you want.  For the sake of this tutorial, we will work with synthetic data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The philosiphy of the package is to make it as accesible as possible. Association of certain parts to a given modality get tracked with simple python dictionaries.\n",
    "These map a string key to the data in a simple numpy array. The model also only supports paired data at the moment, meaning that each row of each modality should be associated to the same sample (e.g. compound).<br>\n",
    "**Please note that preprocessing the data an making sure all arrays are in the correct order is your due diligence!**\n",
    "<br>For our the tutorial we will only use some randomly generated data to keep things fast and."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the random generator\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# create some latent information common to all modalities\n",
    "train_latent_information = rng.random(size = (100, 25))\n",
    "\n",
    "# small helper function for our synthetic data\n",
    "def generate_modality(latent_information: np.array, n_random_dims: int, samples: int = 100):\n",
    "    ar = np.concatenate((latent_information, rng.random(size = (100, n_random_dims))), axis=1)\n",
    "    rng.shuffle(ar, axis=1)\n",
    "\n",
    "    return ar\n",
    "\n",
    "# define the data dictionary\n",
    "# this will be the first part you'll need to hold your actual data\n",
    "train_data_dict = {\n",
    "    \"modality_1\": generate_modality(train_latent_information, 25),\n",
    "    \"modality_2\": generate_modality(train_latent_information, 50),\n",
    "    \"modality_3\": generate_modality(train_latent_information, 75)}\n",
    "\n",
    "\n",
    "# we will also create a separate validation data set sharing some similarity to the training data\n",
    "val_latent_information = train_latent_information * 0.8 + rng.random(size = (100, 25)) * 0.2\n",
    "val_data_dict = {\n",
    "    \"modality_1\": generate_modality(val_latent_information, 25),\n",
    "    \"modality_2\": generate_modality(val_latent_information, 50),\n",
    "    \"modality_3\": generate_modality(val_latent_information, 75)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting up the models\n",
    "### 3.1 Autoencoders\n",
    "The individual autoencoder models also get associated with a modality via a python dictionary. **Please make sure that for each entry in the model dictionary, there exists an entry with the same key in the data dictionary.**<br>\n",
    "\n",
    "The VariationalAutoencoder class is a one-stop-shop do define everything for an autoencoder to work. It expects a few inputs:\n",
    "- encoder: object of class encoder\n",
    "- decoder: object of class decoder\n",
    "- optimizer: string name of optimizer to use (adam, sgd)\n",
    "- learning_rate: learning rate to use for this model\n",
    "- pretrain_epochs: number of epochs to train this model alone\n",
    "- train_joint: boolean if the model should be further trained during the joint training phase\n",
    "- optimizer arguments: any further arguments needed for the optimizer can be passed as keyword arguments\n",
    "\n",
    "The encoder and decoder themselves expect some inputs. These might change in the future in case of more flexble implementations:\n",
    "\n",
    "#### encoder:\n",
    "- n_input: number of features of the input\n",
    "- n_hidden: number of nodes/channels in the hidden layers\n",
    "- n_layers: number of hidden layers of the encoder\n",
    "\n",
    "#### decoder:\n",
    "- n_input: number of features of the input\n",
    "- n_hidden: number of nodes/channels to use in the hidden layers\n",
    "- n_z: number of features in the latent space\n",
    "- n_layers: number of hidden layers of the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"modality_1\": VariationalAutoencoder(DynamicEncoder(50, 42, 2), DynamicDecoder(50, 42, 36, 2), \"adam\", 0.001),\n",
    "    \"modality_2\": VariationalAutoencoder(DynamicEncoder(75, 50, 2), DynamicDecoder(75, 50, 36, 2), \"adam\", 0.001),\n",
    "    \"modality_3\": VariationalAutoencoder(DynamicEncoder(100, 75, 2), DynamicDecoder(100, 75, 36, 2), \"adam\", 0.001)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Support models\n",
    "The multimodal translation architecture works with two support models: a latent space discriminator and a sample classifier. While the discriminator is mandatory for the model to work, the classifier is optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Discriminator\n",
    "The discriminator helps to align the latent spaces of the different modalities. Similar to the autoencoder class, the discriminator already contains its own optimizer. At the moment it is realized as a feed forward neural network.\n",
    "\n",
    "Discriminator:\n",
    "- optimizer: string name of optimizer to use (adam, sgd)\n",
    "- learning_rate: learning rate to use for this model\n",
    "- n_z: number of features in the latent space\n",
    "- n_out: classes to predict (usually the number of modalities)\n",
    "- n_hidden: number of nodes/channels in the hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator(\"adam\", 0.001, 36, len(model_dict), 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Classifier \n",
    "The classifier can support the autoencoders by providing additinal information on how to structure the latent space.\n",
    "\n",
    "Classifier:\n",
    "- optimizer: string name of optimizer to use (adam, sgd)\n",
    "- learning_rate: learning rate to use for this model\n",
    "- n_z: number of features in the latent space\n",
    "- n_out: classes to predict (the number of sample classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster labels** <br>\n",
    "First we need to set up some labels for the classifier to predict. We will assume two classes in the data. Cluster labels are given as a numpy array with the same size as the input data\n",
    "each label in the numpy array is the label for the samples at this index in the training data. The labels can be strings or integers and will be converted to the needed format internally. **Make sure to not use an ndarray.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data = np.concatenate((np.repeat(0, 50), np.repeat(1, 50))).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can initialize the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SimpleClassifier(\"adam\", 0.001, 36, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The JointTraner or the brains of the operation\n",
    "The JointTraner is the major class taking care of training the models and making the models available to use afterwards. It makes use of all the components combined so far and calls everything as needed. It has the following parameters:\n",
    "\n",
    "- model_dict: dictionary holding the intialized models\n",
    "- discriminator: intialized discrimnator object\n",
    "- classifier: intialized classifier object (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = JointTrainer(\n",
    "        model_dict = model_dict,\n",
    "        discriminator = discriminator,\n",
    "        classifier = classifier)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Launching Training\n",
    "Starting the training procedure is done with a single function call to the model. The training call accpets the following inputs:\n",
    "- train_data_dict: data dictionary containing the training data\n",
    "- val_data_dict: data dictionary containing the validation data\n",
    "- batch_size: integer for desired batch size\n",
    "- max_epochs: the maximal number of epochs to train for\n",
    "- recon_weight: multplier for the autoencoder reconstruction loss (either integer or dictionary of string to integer for per model scaling)\n",
    "- beta: float value for influence of variational loss on total loss\n",
    "- disc_weight: float value for influence of discrimnator on total loss\n",
    "- anchor_weight: float value for influence of mean absolute error between latent samples on total loss\n",
    "- cl_weight: float value for influence of sample classifier on total loss (optional)\n",
    "- cluster_labels: nummpy array of cluster labels (optional, only add if classifier was added in model initialization)\n",
    "- log_path: string path to store training metric log to (optional, spills to console if not provided)\n",
    "- use_gpu: boolean if gpu acceleration should be enabled\n",
    "\n",
    "The train call will return a meter dictionary. It contains meter objects for each loss that gets tracked during training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meter_dict = model.train(\n",
    "    train_data_dict = train_data_dict,\n",
    "    val_data_dict = val_data_dict,\n",
    "    batch_size = 10,\n",
    "    max_epochs = 10,\n",
    "    recon_weight = 3,\n",
    "    beta = 0.001,\n",
    "    disc_weight = 3,\n",
    "    anchor_weight = 1,\n",
    "    cl_weight = 3,\n",
    "    cluster_labels = cluster_data,\n",
    "    use_gpu = False)\n",
    "\n",
    "print(meter_dict[\"loss\"].avg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Using the model\n",
    "Once you have trained the model and are satisfied with the parameters you have chosen, you can use the model in inference. For that the JointTrainer provides two functions: `forward` and `translate`.<br>\n",
    "Forward allows you to encode some data using a specific model, e.g. if you are interested in the latent representation. It also returns the reconstruction if you want to do further quality checks.<br>\n",
    "Translate allows you to translate some data from one modality to another. You will need to provide the names of the *from* and *to* model you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's create a completely new numpy array\n",
    "inference_data = rng.random(size = (100, 50))\n",
    "\n",
    "# we can encode this array with the modality_1 model\n",
    "reconstructed_inference, encoded_inference = model.forward(\"modality_1\", inference_data)\n",
    "print(f\"Shape of inference data: {inference_data.shape}\")\n",
    "print(f\"Shape of reconstructed inference data: {reconstructed_inference.shape}\")\n",
    "print(f\"Shape of encoded inference data: {encoded_inference.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also translate from modality_1 to modality_3\n",
    "\n",
    "translated_inference = model.translate(\"modality_1\", \"modality_3\", inference_data)\n",
    "\n",
    "modality_3_shape = train_data_dict[\"modality_3\"].shape\n",
    "print(f\"Shape of modality_3 data: {modality_3_shape}\")\n",
    "print(f\"Shape of inference data: {inference_data.shape}\")\n",
    "print(f\"Shape of translated inference data: {translated_inference.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Saving the model\n",
    "Once you are done trainig and using the model you might want to store it for later use or documentation. The JointTrainer provideds the `save_model` function to store all parts of the model. All it needs is the path to a directory to store everything into. The path will be created if it does not exist yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"./test_save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Loading an existing model\n",
    "If you want to resume the training on an existing model or use a pretrained model for inference, you can provide the path to a stored model when initializing a joint trainer. **You will need to provide the model dict and support models again with the same parameters.** The newly defined objects will be primed with the stored weights internally. **Please make sure to use the same string keys in the model dictionary.** If you don't know the specific details for this checkpoint, please locate the README file inside the checkpoint directory containing all necessary information. **The optimizers will be set up automatically to the state of the checkpoint.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = JointTrainer(\n",
    "        model_dict = model_dict,\n",
    "        discriminator = discriminator,\n",
    "        classifier = classifier,\n",
    "        checkpoint_dir = \"./test_save\")\n",
    "\n",
    "reconstructed_inference, encoded_inference = model.forward(\"modality_1\", inference_data)\n",
    "print(f\"Shape of inference data: {inference_data.shape}\")\n",
    "print(f\"Shape of reconstructed inference data: {reconstructed_inference.shape}\")\n",
    "print(f\"Shape of encoded inference data: {encoded_inference.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7756088dfae82ffe8963c3b0101f571fbcee16e9508b2375e6762c7d087f0f1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
